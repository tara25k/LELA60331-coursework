{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS AND LOADING DATA\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# load in data\n",
    "reviews=[]\n",
    "sentiment_ratings=[]\n",
    "product_types=[]\n",
    "helpfulness_ratings=[]\n",
    "\n",
    "with open(\"Compiled_Reviews.txt\") as f:\n",
    "   for line in f.readlines()[1:]:\n",
    "        fields = line.rstrip().split('\\t')\n",
    "        reviews.append(fields[0])\n",
    "        sentiment_ratings.append(fields[1])\n",
    "        product_types.append(fields[2])\n",
    "        helpfulness_ratings.append(fields[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAM OF POSITIVES V NEGATIVES\n",
    "coded_sentiment_ratings = [1 if x == 'positive' else 0 for x in sentiment_ratings]\n",
    "num_positive = sum(coded_sentiment_ratings)\n",
    "num_negative = len(coded_sentiment_ratings) - num_positive\n",
    "categories = ['Positives', 'Negatives']\n",
    "counts = [num_positive, num_negative]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(categories, counts)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Positives vs. Negatives')\n",
    "plt.show()\n",
    "\n",
    "print(f'{(num_positive/len(coded_sentiment_ratings))*100}% positive')\n",
    "print(f'{(num_negative/len(coded_sentiment_ratings))*100}% negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION FOR REMOVING STEMS\n",
    "def stem_word(word):\n",
    "    suffixes = ['ing', 'ed', 'ly', 'es', 's', 'ment', 'tion', 'er', 'est']\n",
    "    suffixes = sorted(suffixes, key=len, reverse=True)\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]  \n",
    "    return word  \n",
    "\n",
    "# LOAD IN STOPWORDS\n",
    "f = open(\"data/stopwords.txt\", \"r\")\n",
    "stopwords = []\n",
    "for line in f:\n",
    "    stopwords.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "punct = ['#', '\"', '\"\"', '%', '$', '&', ')', '(', '+', '*', '-'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN STOPWORDS\n",
    "f = open(\"data/stopwords.txt\", \"r\")\n",
    "stopwords = []\n",
    "for line in f:\n",
    "    stopwords.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "punct = ['#', '\"', '\"\"', '%', '$', '&', ')', '(', '+', '*', '-'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY - BASE MODEL\n",
    "\n",
    "# tokenise reviews\n",
    "token_def = re.compile(\"[^ \\.?!:,)(\\\"]+\")\n",
    "tokenized_sents = [token_def.findall(txt) for txt in reviews]\n",
    "tokens=[]\n",
    "for s in tokenized_sents:\n",
    "\n",
    "    # FOR BASE MODEL\n",
    "    tokens.extend(s)\n",
    "\n",
    "# use a counter to count the tokens\n",
    "counts=Counter(tokens)\n",
    "# sort the tokens\n",
    "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "so=list(zip(*so))[0]\n",
    "\n",
    "# get 5000 most common into vocabulary\n",
    "type_list=so[0:5000]\n",
    "vocab_list = type_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY - DOWNLOADED VOCAB\n",
    "\n",
    "# ENGLISH DICTIONARY WORDS\n",
    "f = open(\"data/popular.txt\", \"r\")\n",
    "tokens = []\n",
    "for line in f:\n",
    "    tokens.append(line.strip())\n",
    "f.close()\n",
    "vocab_list = tokens\n",
    "\n",
    "# POSITIVE AND NEGATIVE SENTIMENT WORDS\n",
    "f = open(\"data/positive-words.txt\", \"r\")\n",
    "positive = []\n",
    "for line in f:\n",
    "    positive.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/negative-words.txt\", \"r\")\n",
    "negative = []\n",
    "for line in f:\n",
    "    negative.append(line.strip())\n",
    "f.close()\n",
    "tokens = positive + negative\n",
    "vocab_list = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY - REMOVED STOPWORDS\n",
    "\n",
    "# tokenise reviews\n",
    "token_def = re.compile(\"[^ \\.?!:,)(\\\"]+\")\n",
    "tokenized_sents = [token_def.findall(txt) for txt in reviews]\n",
    "tokens=[]\n",
    "for s in tokenized_sents:\n",
    "    filtered_tokens = [t.lower() for t in s if t.lower() not in stopwords and t not in punct]\n",
    "    tokens.extend(filtered_tokens)\n",
    "\n",
    "\n",
    "# use a counter to count the tokens\n",
    "counts=Counter(tokens)\n",
    "# sort the tokens\n",
    "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "so=list(zip(*so))[0]\n",
    "\n",
    "# get 5000 most common into vocabulary\n",
    "type_list=so[0:5000]\n",
    "vocab_list = type_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY - STEM WORDS\n",
    "\n",
    "# tokenise reviews\n",
    "token_def = re.compile(\"[^ \\.?!:,)(\\\"]+\")\n",
    "tokenized_sents = [token_def.findall(txt) for txt in reviews]\n",
    "tokens=[]\n",
    "for s in tokenized_sents:\n",
    "    tokens.extend(s)\n",
    "\n",
    "\n",
    "# FOR REMOVING STEMS\n",
    "pre_stem_tokens = [stem_word(t) for t in tokens if t not in stopwords and t not in punct]\n",
    "tokens = list(set(pre_stem_tokens))\n",
    "\n",
    "# use a counter to count the tokens\n",
    "counts=Counter(tokens)\n",
    "# sort the tokens\n",
    "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "so=list(zip(*so))[0]\n",
    "\n",
    "# get 5000 most common into vocabulary\n",
    "type_list=so[0:5000]\n",
    "vocab_list = type_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY - BIGRAMS\n",
    "\n",
    "# tokenise reviews\n",
    "token_def = re.compile(\"[^ \\.?!:,)(\\\"]+\")\n",
    "tokenized_sents = [token_def.findall(txt) for txt in reviews]\n",
    "tokens=[]\n",
    "for s in tokenized_sents:\n",
    "    tokens.extend(tokens)\n",
    "    tokens.extend([f\"{tokens[i]}_{tokens[i+1]}\" for i in range(len(tokens) - 1)]) \n",
    "\n",
    "# use a counter to count the tokens\n",
    "counts=Counter(tokens)\n",
    "# sort the tokens\n",
    "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "so=list(zip(*so))[0]\n",
    "\n",
    "# get 5000 most common into vocabulary\n",
    "type_list=so[0:5000]\n",
    "vocab_list = type_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AN EMBEDDING MATRIX\n",
    "# # rerun this if you change the vocab\n",
    "M = np.zeros((len(reviews), len(vocab_list)))\n",
    "print(len(reviews))\n",
    "for i, rev in enumerate(reviews):\n",
    "    if i%1000 == 0:\n",
    "         print(i)\n",
    "\n",
    "    # FOR BASE MODEL\n",
    "    tokens = [t for t in token_def.findall(rev)]\n",
    "\n",
    "    # FOR LOWERCASING AND REMOVING STOPWORDS\n",
    "    # tokens = [t.lower() for t in token_def.findall(rev) if t.lower() not in stopwords and t not in punct]\n",
    "    \n",
    "    # FOR BIGRAMS\n",
    "    # tokens = [t for t in token_def.findall(rev)]\n",
    "    # bigrams = [f\"{tokens[j]}_{tokens[j+1]}\" for j in range(len(tokens) - 1)]\n",
    "    # all_tokens = tokens + bigrams \n",
    "\n",
    "    # FOR REMOVING STEMS \n",
    "    # tokens = [stem_word(token) for token in tokens]\n",
    "    \n",
    "    # iterate over vocab\n",
    "    for j, vocab_token in enumerate(vocab_list):\n",
    "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
    "        if vocab_token in tokens:\n",
    "              \n",
    "              # FOR ONE HOT\n",
    "              M[i,j] = 1\n",
    "              \n",
    "              # FOR BAG OF WORDS\n",
    "            #   M[i, j] += 1\n",
    "\n",
    "\n",
    "\n",
    "train_ints=np.random.choice(len(reviews),int(len(reviews)*0.8),replace=False)\n",
    "test_ints=list(set(range(0,len(reviews))) - set(train_ints))\n",
    "M_train = M[train_ints,]\n",
    "M_test = M[test_ints,]\n",
    "\n",
    "# for labels, use a vector representation\n",
    "labels_train = [sentiment_ratings[i] for i in train_ints]\n",
    "labels_test = [sentiment_ratings[i] for i in test_ints]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE MODEL \n",
    "y=[int(l == \"positive\") for l in labels_train]\n",
    "y = np.array(y)\n",
    "num_features=len(vocab_list)\n",
    "weights = np.random.rand(num_features)\n",
    "bias=np.random.rand(1)\n",
    "n_iters = 6000\n",
    "lr=0.225\n",
    "logistic_loss=[]\n",
    "num_samples=len(y)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    if i % 100 == 0: # for logging progress\n",
    "        print(i)\n",
    "    \n",
    "    loss = 0.0\n",
    "    z = M_train.dot(weights.T) + bias\n",
    "    q = 1/(1+np.exp(-z))\n",
    "    eps=0.00001\n",
    "    loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
    "    logistic_loss.append(loss)\n",
    "\n",
    "    # BASE MODEL\n",
    "    dw = ((q-y).dot(M_train) * (1/len(y)))\n",
    "\n",
    "    # L2 REGULARISATION\n",
    "    # dw = ((q-y).dot(M_train) * (1/len(y))) + (0.001*weights) \n",
    "    \n",
    "    db = sum((q-y))/len(y) \n",
    "    weights = weights - lr*dw \n",
    "    bias = bias - lr*db\n",
    "\n",
    "    \n",
    "\n",
    "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL WITH L2 REGULARISATION\n",
    "y=[int(l == \"positive\") for l in labels_train]\n",
    "y = np.array(y)\n",
    "num_features=len(vocab_list)\n",
    "weights = np.random.rand(num_features)\n",
    "bias=np.random.rand(1)\n",
    "n_iters = 6000\n",
    "lr=0.225\n",
    "logistic_loss=[]\n",
    "num_samples=len(y)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    if i % 100 == 0: # for logging progress\n",
    "        print(i)\n",
    "    \n",
    "    loss = 0.0\n",
    "    z = M_train.dot(weights.T) + bias\n",
    "    q = 1/(1+np.exp(-z))\n",
    "    eps=0.00001\n",
    "    loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
    "    logistic_loss.append(loss)\n",
    "\n",
    "    # BASE MODEL\n",
    "    dw = ((q-y).dot(M_train) * (1/len(y)))\n",
    "\n",
    "    # L2 REGULARISATION\n",
    "    # dw = ((q-y).dot(M_train) * (1/len(y))) + (0.001*weights) \n",
    "    \n",
    "    db = sum((q-y))/len(y) \n",
    "    weights = weights - lr*dw \n",
    "    bias = bias - lr*db\n",
    "\n",
    "    \n",
    "\n",
    "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL WITH BATCH TRAINING\n",
    "\n",
    "k = 443\n",
    "# Create array of all indices in training data\n",
    "a=np.arange(M_train.shape[0])\n",
    "# randomly shuffle indices in place (in case of classes being unequally distributed across positions in data)\n",
    "np.random.shuffle(a)\n",
    "# Split indices into k equal batches\n",
    "batches=np.array(np.split(a, k))\n",
    "\n",
    "y=[int(l == \"positive\") for l in labels_train]\n",
    "y = np.array(y)\n",
    "num_features=len(vocab_list)\n",
    "weights = np.random.rand(num_features)\n",
    "bias=np.random.rand(1)\n",
    "n_iters = 6000\n",
    "lr=0.225\n",
    "logistic_loss=[]\n",
    "num_samples=len(y)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    if i % 100 == 0: # for logging progress\n",
    "        print(i)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for j in range(len(batches)):\n",
    "        this_batch_M_train = M_train[batches[j]]\n",
    "        this_batch_y_train = y[batches[j]]\n",
    "        z = this_batch_M_train.dot(weights.T) + bias\n",
    "        q = 1/(1+np.exp(-z))\n",
    "\n",
    "        # calculate loss\n",
    "        eps=0.00001\n",
    "        loss = -sum((this_batch_y_train*np.log2(q+eps)+(np.ones(len(this_batch_y_train))-this_batch_y_train)*np.log2(np.ones(len(this_batch_y_train))-q+eps)))\n",
    "        \n",
    "\n",
    "        # calculate gradients\n",
    "        dw = ((q-this_batch_y_train).dot(this_batch_M_train) * (1/len(this_batch_y_train)))\n",
    "        db = sum((q-this_batch_y_train))/len(this_batch_y_train) \n",
    "\n",
    "        # update weights\n",
    "        weights = weights - lr*dw \n",
    "        bias = bias - lr*db\n",
    "    logistic_loss.append(loss)\n",
    "    \n",
    "\n",
    "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "#loss = sum(-(np.ones(len(y))*np.log2(q)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE ON TEST SET\n",
    "z = M_test.dot(weights)+bias # weighted inputs\n",
    "q = 1/(1+np.exp(-z)) # sigmoided input\n",
    "\n",
    "y_test_pred = [int(prob > 0.5) for prob in q] \n",
    "y_test=[int(l == \"positive\") for l in labels_test]\n",
    "\n",
    "\n",
    "# EVALUATION METRICS\n",
    "# accuracy\n",
    "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
    "print(f'accuracy: {sum(acc_test)/len(acc_test)}')\n",
    "\n",
    "# precision and recall\n",
    "labels_test_pred=[\"positive\" if s == 1 else \"negative\" for s in y_test_pred]\n",
    "true_positives=sum([int(yp == \"positive\" and labels_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])\n",
    "false_negatives=sum([int(yp == \"negative\" and labels_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])\n",
    "false_positives=sum([int(yp == \"positive\" and labels_test[s] == \"negative\") for s,yp in enumerate(labels_test_pred)])\n",
    "true_negatives=sum([int(yp == \"negative\" and labels_test[s] == \"negative\") for s,yp in enumerate(labels_test_pred)])\n",
    "\n",
    "precision = true_positives/(true_positives + false_positives)\n",
    "recall = true_positives/(true_positives + false_negatives)\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMINING WEIGHTS\n",
    "print(\"most impactful words for a positive review:\")\n",
    "print([vocab_list[x] for x in np.argsort(weights)[::-1][:20]])\n",
    "print([vocab_list[x] for x in np.argsort(weights)[::-1][20:40]])\n",
    "\n",
    "print(\"\\nmost impactful words for a negative review:\")\n",
    "print([vocab_list[x] for x in np.argsort(weights)[:20]])\n",
    "print([vocab_list[x] for x in np.argsort(weights)[20:40]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
